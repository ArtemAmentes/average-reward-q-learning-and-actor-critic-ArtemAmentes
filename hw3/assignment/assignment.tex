\documentclass[12pt, oneside]{article}
\usepackage[T2A]{fontenc}

\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage[
  letterpaper,
  left=2.5cm,
  right=2.5cm,
  top=2.5cm,
  bottom=1.5cm
]{geometry}

\usepackage[unicode]{hyperref}
\hypersetup{
    colorlinks=true,
    % allcolors=black
}

\usepackage{fancyhdr}
\fancyhf{}
\pagestyle{fancy}
\fancyhead[L]{MIPT CDS}
\fancyhead[C]{Reinforcement Learning}
\fancyhead[R]{Spring 2023}

\author{Due May 10}
\title{Assignment 3}
\date{}


\begin{document}
\maketitle
\thispagestyle{fancy}

\hyphenation{диф-фе-рен-ци-ал}
\section{Теоретическая часть: среднее вознаграждение}

Данное теоретическое задание посвящено таким понятиям как среднее вознаграждение и дифференциал полезности.

\hyphenation{дей-ствия}
\textbf{Задание 1. Упражнение 10.6 книги Саттона и Барто}. Расмотрим марковский процесс (примечание: просто процесс, не МППР!), состоящий из трех состояний A, B, C; переход производятся детерминированно по циклу. Вознаграждение $+1$ начисляется по прибытии в состояние A, в остальных случаях оно равно 0. Каковы дифференциалы для полезностей во всех трех состояний?

\hyphenation{оце-ни-ва-ния}
\hyphenation{пос-ле-до-ва-тель-нос-ть}
\hyphenation{воз-наг-раж-де-ния}
\hyphenation{сред-не-го}
\hyphenation{пред-по-ло-жим}
\textbf{Задание 2. Упражнение 10.8 из книги Саттона и Барто.} Псевдокод на стр.296-297 (над упражнением 10.6 --- Дифференциальный полуградиентный Sarsa для оценивания $\hat{q} = q_*$) обновляет $\hat{R}_{t+1}$, используя в качестве ошибки $\delta_t$, а не просто $R_{t+1} - \hat{R}_{t+1}$. Годятся оба определения ошибки, но $\delta_t$ дает лучшие результаты. Чтобы понять, почему, рассмотрим круговой МППР с тремя состояниями из задания 1. Оценка среднего вознаграждения должна стремиться к своему истинному значению $\frac{1}{3}$. Предположим, что она уже принимала это значение и там и осталась. Какой тогда была бы последовательность $R_t - \hat{R}_t$? А какой была бы последовательность $\delta_t$ (вычисленная по формуле 10.10)? Какая последовательность давала бы более устойчивую оценку среднего вознаграждения, если бы оценке было разрешено изменяться в ответ на ошибки?

\section{Практическая часть: Q-обучение и актор-критик}

Цель задания --- реализовать и поэкспериментировать с методами Q-обучения и актор-критик.

Данное задание основано на третьем задании курса \href{http://rail.eecs.berkeley.edu/deeprlcourse/}{Deep RL Университета Беркли}.

Код задания расположен по адресу
\begin{center}
    \href{https://github.com/pkuderov/mipt-rl-hw-2023}{https://github.com/pkuderov/mipt-rl-hw-2023}
\end{center}

Как и в предыдущих заданиях, у вас есть возможность выполнять его как локально, так и в Google Colab. Подробности, в том числе по установке зависимостей, вы найдете в \verb|README| к третьему заданию в репозитории.

\subsection{Часть 1. Q-обучение}

Часть 1 практического задания требует, чтобы вы реализовали и оценили качество работы Q-обучения для игры Atari. Так как алгоритм Q-обучения был рассмотрен на лекциях и семинарах, вам будет предоставлен стартовый код.

Это задание можно выполнить и на CPU, хотя, как обычно, на GPU будет работать быстрее. В качестве альтернативы можете воспользоваться Google Colab.

\subsubsection{Файлы}

\hyphenation{до-маш-них}
\hyphenation{не-об-хо-ди-мые}
Код задания основан на коде, который вы реализовали в первых двух заданиях. Необходимые для запуска файлы находятся в папке \verb|hw3|. Требуется заполнить своим решением из домашних заданий 1 и 2 места, отмеченные \verb|# TODO: get this from hw1 or hw2|, в следующих файлах:

\begin{itemize}
    \item \verb|infrastructure/rl_trainer.py|,
    \item \verb|infrastructure/utils.py|,
    \item \verb|policies/MLP policy.py|
\end{itemize}

Ваша реализация Q-обучения будет расположена в следующих файлах:

\begin{itemize}
    \item \verb|agents/dqn_agent.py|,
    \item \verb|critics/dqn_critic.py|,
    \item \verb|policies/argmax policy.py|
\end{itemize}

\subsubsection{Реализация Q-обучения}

Первый этап задания заключается в реализации рабочей версии Q-learning. Код по умолчанию запустит игру \textit{Ms. Pac-Man} с разумными настройками гиперпараметров. Найдите маркеры \verb|# TODO| в перечисленных выше файлах для получения инструкций по реализации. Вы можете заглянуть внутрь \verb|infrastructure/dqn_utils.py|, чтобы понять, как работает (оптимизированный для памяти) буфер воспроизведения. Не требуется его модифицировать.

\hyphenation{ги-пер-па-ра-мет-ров}
Для ответа на некоторые вопросы в задании может потребоваться донастройка гиперпараметров, архитектуры нейронной сети и даже смена игры. Это должно быть сделано путем изменения аргументов командной строки, переданных в \verb|run_hw3_dqn.py|, или путем изменения параметров запука скрипта в Colab.

\hyphenation{ре-зуль-та-та}
Чтобы определить, верна ли ваша реализация Q-learning, вы должны запустить ее с гиперпараметрами по умолчанию в игре \textit{Ms. Pac-Man} на 1 миллион шагов, используя команду к соответствующему заданию из \verb|README|. Эталонное решение достигает результата в 1500 за этот период. В Colab это займет примерно 3 часа GPU. Если вы видите, что обучение занимает намного больше времени, возможно, в вашей реализации есть ошибка.

Мы рекомендуем отладить ваше решение сначала в среде \textit{LunarLander-v3}. Эталонное решение с гиперпараметрами по умолчанию достигает результата около 150 после 350 тыс шагов в среде. Однако нужно быть готовым к большой дисперсии результатов. Без трюка с двойным Q-обучением средняя отдача часто уменьшается после достижения результата 150.

\subsubsection{Тестирование Q-обучения}

\hyphenation{по-лу-чить}
После того, как у вас есть работающая реализация Q-learning, выполните указанные ниже эксперименты и подготовьте отчет. Для каждого задания ниже ожидается получить от вас по одному графику.

\textbf{Задание 1. Качество базовой версии Q-обучения (DQN)}

\hyphenation{до-маш-не-му}
\hyphenation{ви-зу-али-зи-ро-вать}
Включите график кривой обучения на \textit{Ms. Pac-Man}. Ось X должна соответствовать количеству временных шагов, а ось Y --- показывать среднее вознаграждение за эпоху, а также лучшее среднее вознаграждение на данный момент. Расчет данных величин уже есть в начальном коде. Как и в предыдущих заданиях, вы можете визуализировать их с помощью Tensorboard. Чтобы получить хорошую производительность, вам не нужно изменять гиперпараметры по умолчанию. Если изменяете какие-либо гиперпараметры, перечислите их, пожалуйста, в подписи к графику.

Для запуска эксперимента используйте строку, указанную на странице \verb|README| к третьему домашнему заданию (секция \textit{Experiment 1. Base Q-learning})

\textbf{Задание 2. Качество двойного Q-обучения (DDQN)}

\hyphenation{ис-поль-зо-ва-ние}
\hyphenation{слу-чай-ных}
Используйте двойное Q-обучение для повышения точности обучаемых Q-значений за счет снижения их переоценки, свойственных обычному Q-обучению [из-за операции взятия максимума в формуле оценки]. Данная модификация предполагает использование онлайновой Q-сети [вместо целевой] для выбора \textbf{наилучшего действия} при вычислении целевых значений. Сравните производительность DDQN с базовым DQN. Проведите как минимум три запуска с различными инициализациями генератора случайных чисел как для DQN, так и для DDQN. Вы можете использовать \textit{LunarLander-v3} в этом задании. Для запуска эксперимента используйте строки запуска, указанные на странице \verb|README| к третьему домашнему заданию (секция \textit{Experiment 2. DQN vs DDQN}).

Приложите к сдаче логи данных запусков. В отчете приведите график со сравнением средних результатов алгоритмов.

\textbf{Задание 3. Эксперименты с гиперпараметрами}

\hyphenation{раз-лич-ны-ми}
\hyphenation{про-из-воль-но}
\hyphenation{гипер-пара-мет-ра}
\hyphenation{гипер-пара-мет-р}
\hyphenation{ука-зан-ные}
\hyphenation{про-из-во-ди-тель-ность}
\hyphenation{гра-фи-ке}
\hyphenation{ис-поль-зо-ван}
Теперь проанализируем чувствительность Q-обучения к гиперпараметрам. Произвольно выберите один гиперпараметр и запустите алгоритм с не менее чем тремя различными его значениями в дополнение к тому значению, который был использован ранее в задании 1. Приведите график с кривыми обучения для всех четырех значений. В подписи поясните ваш выбор и опишите влияние этого гиперпараметра на производительность. Для запуска эксперимента используйте строки запуска, указанные в \verb|README| к третьему домашнему заданию (секция \textit{Experiment 3. DQN hyperparameters}). Вы можете взять в качестве тестовой среды что-то отличное от Lunar Lander.

\hyphenation{ней-рон-ной}
\hyphenation{аль-тер-на-ти-ву}
\hyphenation{же-ла-тель-но}
Примеры гиперпараметров для исследования: скорость обучения; архитектура нейронной сети, например, количество слоев, размер скрытого слоя и т. д.; расписание исследования или стратегия исследования (например, вы можете реализовать альтернативу эпсилон-жадности и установить разные значения гиперпараметров) и т. д. Желательно сначала поэкспериментировать с разными гиперпараметрами, чтобы найти и выбрать такой параметр, который существенно повлияет на производительность.

\subsection{Часть 2. Актор-критик}

\hyphenation{прош-лом}
\hyphenation{об-ра-ти-те}
Во второй часть практического задания потребуется изменить реализованный во втором домашнем задании алгоритм градиента стратегии до метода актор-критик. Обратите внимание, что обучение критика может занять больше времени, чем обучение актора.

Вспомним, как мы считали градиент стратегии в прошлом задании:

\begin{align}
    &\nabla_{\Theta} J(\Theta) \approx \mathbf{E} \left[ \sum_{t=1}^T \nabla_{\Theta} \log \pi_{\Theta} (a_t | s_t) A^{\pi}(s_t, a_t) \right], \\
    &A^{\pi}(s_t, a_t) \approx \left( \sum_{t'=t}^T r(s_{t'}, a_{t'}) \right) - V_{\Phi}^{\pi}(s_t)
\end{align}

\hyphenation{по-лез-ность}
\noindent
где мы использовали ``rewards-to-go'' оценку Q-функции и дополнительно вычитали полезность состояния в качестве базового уровня, тем самым получая оценку функции преимущества $A^{\pi}(s_t, a_t)$.

На практике такая оценка значения преимущества отличается высокой дисперсией. В методе актор-критик решается проблема высокой дисперсии путем использования сети критика для оценки полезности. Существуют разные варианты, какую функцию полезности оценивать критиком. Один из них --- функцию полезности состояния. Тогда оценка преимущества может быть задана следующим образом:

\begin{equation}
    A^{\pi}(s_t, a_t) \approx r(s_t, a_t) + \gamma V_{\Phi}^{\pi}(s_{t+1}) - V_{\Phi}^{\pi}(s_t)
\end{equation}

\hyphenation{ошиб-ку}
\hyphenation{пред-ла-га-ет-ся}
В данном задании в качестве основы для сети критика мы будем использовать ту же сеть оценки полезности, что и во втором домашнем задании. Обучать же критика предлагается через минимизацию среднеквадратичной TD-ошибки по стандартному TD-правилу:

\begin{align}
    y_t &= r(s_t, a_t) + \gamma V_{\Phi}^{\pi}(s_{t+1}), \\
    &\min_{\Phi} \text{MSE} (V_{\Phi}^{\pi}(s_t) - y_t)
\end{align}

\hyphenation{рав-но-силь-но}
\hyphenation{обу-че-ния}
\hyphenation{зна-че-ни-ям}
В теории, шаг минимизации необходимо выполнять каждый раз, когда обновляется стратегия агента, чтобы стратегия и функция полезности, которая ее оценивает, были синхронизированы. Однако на практике эта операция обычно чересчур дорогостоящая. Компромиссным решением будет выполнять лишь несколько шагов градиентного спуска для обновления критика на каждой итерации (похожей логикой мы руководствовались, когда на лекциях разбирали переход от Policy Iteration к Value Iteration). Обратите внимание, что это не равносильно кратному увеличению гиперпараметра скорости обучения, так как на каждом шаге градиентного спуска (или раз в несколько шагов, как в текущем задании) мы будем заново пересчитывать целевые значения. В целом, процесс обучения критика представляет собой итеративный процесс, в котором мы постоянно обновляем его оценку полезности, чтобы она примерно соответствовала целевым значениям для меняющейся стратегии актора.

\subsubsection{Реализация метода актор-критик}

\hyphenation{пот-ре-бу-ет-ся}
\hyphenation{реа-ли-за-цию}
\hyphenation{об-нов-ле-нию}
\hyphenation{кор-рект-но}
\hyphenation{тре-буе-мую}
\hyphenation{за-пол-нить}
Ваш код будет основан решении второго домашнего задания. Вам потребуется заполнить пропуски, помеченные \verb|TODO| для следующих частей кода:

\begin{itemize}
    \item В \verb|policies/MLP_policy.py| реализуйте функцию \verb|update| для класса \verb|MLPPolicyAC|. Следует отметить, что класс стратегии актор-критика фактически совпадает с классом стратегии из домашнего задания по градиенту стратегии (за исключением отсутствия базового уровня).
    \item В \verb|agents/ac_agent.py| дополните функцию \verb|train|. Она должна содержать реализацию всех требуемых шагов по обучению критика, оценке преимущества, а затем обновлению стратегии актора. Залогируйте функцию потерь в конце, чтобы отслеживать ее значения во время обучения.
    \item В \verb|agents/ac_agent.py| дополните функцию \verb|estimate_advantage|. Эта функция использует сеть критика для оценки значения преимущества. Не забудьте корректно обработать окончания эпизода.
    \item В \verb|critics/bootstrapped_continuous_critic.py| заполните пропуски в функции \verb|update|. Обратите внимание на переменные 
    \verb|num_grad_steps_per_target_update| и \verb|num_target_updates|, которые задают число шагов градиентного спуска и требуемую частоту обновления целевых значений в процессе.
\end{itemize}

\subsubsection{Тестирование метода актор-критик}

После того, как у вас есть работающая реализация актор-критика, выполните указанные ниже эксперименты и подготовьте отчет.

\textbf{Задание 4. Проверка правильности реализации на CartPole}

Цель этого задания убедиться в правильности вашей реализации на примере простой среды \textit{CartPole-v0}. Для запуска эксперимента используйте строки запуска, указанные на странице \verb|README| к третьему домашнему заданию (секция \textit{Experiment 4. Sanity check with CartPole}). Вам потребуется сравнить качество работы агента с разными частотами обучения критика и обновления целевых функций. Агент с наилучшим набором гиперпараметров должен полностью решать данную среду, набирая результат 200. Подготовьте отчет со сравнением сделанных запусков в виде графика кривых обучения и кратким текстовым пояснением.

\textbf{Задание 5. Проверка метода актор-критик на более сложных средах}

\hyphenation{про-тес-ти-руй-те}
\hyphenation{за-пус-ка}
\hyphenation{тре-тье-му}
Используя наилучшие значения гиперпараметров из предыдущего задания, протестируйте качество работы вашей реализации в средах \textit{InvertedPendulum} и \textit{HalfCheetah}. Для запуска эксперимента используйте строки запуска, указанные в \verb|README| к третьему домашнему заданию (секция \textit{Experiment 5. Run actor-critic with more difficult tasks}).

Ваши результаты должны примерно совпадать с результатами градиента стратегии из второго домашнего задания. После 150 итераций в \textit{HalfCheetah} результат должен быть около 150. После 100 итераций в \textit{InvertedPendulum} результат должен быть около 1000. В начале обучения отдача должна начать расти немедленно. Например, после 20 итераций отдача в \textit{HalfCheetah} должна быть выше -40, а ваш доход \textit{InvertedPendulum} должен быть около или выше 100. Однако, конечно, нужно учитывать, что результаты между запусками могут сильно варьироваться.

Результатом работы в этом разделе являются графики результатов для каждой из сред.


\section{Формат отправки}

\textit{Формат сдачи совпадает с форматом сдачи первого практического задания.}

\hyphenation{пред-ло-же-нии}
Сдача предполагается в виде коммитов и комментариев к ним в соответствующем предложении изменения кода (pull request) в GitHub classroom (ссылка в тг канале курса).

\hyphenation{не-до-ста-ю-щих}
Ожидается, что сдача будет содержать непосредственно код заполненных вами недостающих частей выданной заготовки решения и логи финальных запусков (для каждого задания и каждой из использованных сред).

\hyphenation{от-лич-ной}
Оригинально все логи лежат в папке \verb|data|. Логи финальных запусков скопируйте из \verb|data| в отдельную папку \verb|run_logs| и отправьте вместе с вашим решением. Отличной альтернативой будет вместо этого снабдить решение ссылкой на отчет (report) в wandb, что потребует самостоятельно интегрировать использование данного сервиса в ваше решение.

\hyphenation{ре-ше-ние}
\hyphenation{аль-тер-на-ти-ва}
Также в сообщении к предложению необходимо добавить результаты, описание и решение по каждому из пунктов задания (в соответствии с тем, что оно требует). Разметка markdown позволяет и вставку картинок, и оформление табличек. Опционально, вы можете оформить результаты в виде отдельного файла .doc или .pdf и добавить их в посылку (commit), а в сообщении сослаться на этот файл. Не забудьте для каждого пункта задания код запуска, чтобы можно было воспроизвести ваши результаты.

\end{document}
