\documentclass[12pt, oneside]{article}
\usepackage[T2A]{fontenc}

\usepackage[
  letterpaper,
  left=2.5cm,
  right=2.5cm,
  top=2.5cm,
  bottom=1.5cm
]{geometry}

\usepackage[unicode]{hyperref}
\hypersetup{
    colorlinks=true,
    % allcolors=black
}

\usepackage{fancyhdr}
\fancyhf{}
\pagestyle{fancy}
\fancyhead[L]{MIPT CDS}
\fancyhead[C]{Reinforcement Learning}
\fancyhead[R]{Spring 2023}

\author{Due April 24}
\title{Assignment 2}
\date{}


\begin{document}
\maketitle
\thispagestyle{fancy}

\section{Теоретическая часть: аппроксимация стратегии}

Данное теоретическое задание посвящено методам аппроксимации стратегии.

\hyphenation{дей-ствия}
\textbf{Задание 1} Воспользуйтесь своими знаниями о клеточном мире и его динамике, чтобы найти точное символьное выражение для оптимальной вероятности выбора действия \textit{right} в примере 13.1 книги Саттона и Барто.

\section{Практическая часть: градиент стратегии}

Цель задания --- реализовать и поэкспериментировать с методом прямой оптимизации стратегии с помощью градиентного спуска и его вариациями.

Данное задание основано на втором задании курса по \href{http://rail.eecs.berkeley.edu/deeprlcourse/}{Deep RL Университета Беркли}.

Код задания расположен по адресу
\begin{center}
    \href{https://github.com/pkuderov/mipt-rl-hw-2023}{https://github.com/pkuderov/mipt-rl-hw-2023}
\end{center}

Как и в первом задании, у вас есть возможность выполнять его как локально, так и в Google Colab. Подробности, в том числе по установке зависимостей, вы найдете в \verb|README| ко второму заданию в репозитории.

\subsection{Обзор реализации}

\subsubsection{Файлы}

\hyphenation{не-за-ви-си-мую}
\hyphenation{по-ме-че-ны}
Для реализации градиента стратегии предлагается частично воспользоваться кодом из первого практического задания. Чтобы каждое практическое задание имело независимую реализацию, переиспользуемые файлы были скопированы в соответствующие директории второго задания. В этих файлах сохранены пропуски, которые вам нужно заполнить своей реализацией из первого задания --- такие пропуски помечены комментарием \verb|# TODO: get this from hw1|.

После заполнения пропусков в компонентнах из первого практического задания, вы можете приступить к работе над кодом градиента стратегии, заполняя пропуски, помеченные \verb|TODO|, в следующих файлах:

\begin{itemize}
    \item \verb|agents/pg_agent.py|,
    \item \verb|policies/MLP_policy.py|
\end{itemize}

По аналогии с первым заданием скрипты запуска экспериментов [локально или в Google Colab] вы найдете в папке \verb|hw2/scripts/|.

\subsubsection{Цикл обучения}

Основной цикл обучения реализован в \verb|infrastructure/rl_trainer.py|, как и в первом практическом задании.

Алгоритм градиента стратегии использует 3 шага:

\begin{itemize}
    \item Сбор траекторий агента в среде с использованием его текущей стратегии.
    \item Оценка отдач для траекторий и вычисление значений функции преимущества (advantage), которые реализованы в функции \verb|train| в \verb|pg_agent.py|.
    \item Шаг обучения параметров стратегии агента. Вычислительный граф (нейронная сеть), задающий стратегию, вычисление базового уровня (baseline), а также код их обновления вы найдете в \verb|policies/MLP_policy.py|.
\end{itemize}

\subsection{Реализация градиента стратегии}

В этой части вам нужно будет реализовать в \verb|pg_agent.py| два варианта оценки отдачи. Первый вариант (``Case 1'' в функции \verb|calculate_q_vals|) использует дисконтированную сумму вознаграждений за всю траекторию и относится к ``vanilla'' версии градиента стратегии:

\begin{equation}
    r(\tau_i) = \sum_{t' = 0}^{T - 1} {\gamma^{t'} r(s_{it'}, a_{it'})}.
\end{equation}

\noindent
Вторая версия (``Case 2'') использует ``reward-to-go'' формулировку:

\begin{equation}
    r(\tau_i) = \sum_{t' = t}^{T - 1} {\gamma^{t'} r(s_{it'}, a_{it'})}.
\end{equation}

\noindent
\textit{Обратите внимание, что она отличается начальным моментом при суммировании.}

Реализуйте также помеченные \verb|TODO| секции, необходимые для запуска небольших экспериментов в следующем пункте задания. А к остальным --- связанным с подсчетом базового уровня --- мы вернемся чуть позже.

\subsection{Запуск простых экспериментов}

\hyphenation{реа-ли-за-ции}
Проведите набор простых экспериментов и оцените, какие параметры и варианты реализации градиента стратегии влияют на скорость и качество его обучения.

\hyphenation{соот-ветствую-щей}
\textbf{Эксперимент 1: CartPole.} Проведите серию экспериментов с методом градиента стратегии в дискретной среде \textit{CartPole-v0}, используя команды для запуска из соответствующей секции \verb|README| ко второму практическому заданию.

Постройте два графика:

\hyphenation{ма-лень-ким}
\hyphenation{боль-шим}
\begin{enumerate}
    \item Кривые обучения средней отдачи на каждой итерации для экспериментов с маленьким размером пакета (batch) --- эксперименты с префиксом \verb|q1_sb_|.
    \item Кривые обучения средней отдачи на каждой итерации для экспериментов с большим размером пакета (batch) --- эксперименты с префиксом \verb|q1_lb_|.
\end{enumerate}

Ответьте письменно в краткой форме на следующие вопросы:

\begin{itemize}
    \item Какой из вариантов оценки отдачи имеет лучшие результаты без нормализации значения преимущества?
    \item Помогает ли нормализация значения преимущества?
    \item Как влияет размер пакета на качество обучения?
\end{itemize}

В пояснениях к данному пункту приведите точные команды, использованные вами для запуска экспериментов.

\hyphenation{мак-си-маль-но-му}
\textit{Ожидается, что корректная реализация метода будет сходиться к максимальному значению результата (200).}

\textbf{Эксперимент 2: InvertedPendulum.} Проведите серию экспериментов с методом градиента стратегии в непрерывной среде \textit{InvertedPendulum-v2}, используя команды для запуска из соответствующей секции \verb|README| вторго практического задания.

\hyphenation{коэф-фи-ци-ент}
\hyphenation{ре-зуль-та-та}
Вашей задачей будет подобрать наименьший размер пакета \verb|b*| и наибольший коэффициент скорости обучения \verb|r*|, при которых алгоритм успевает достичь максимального результата в 1000 за 100 итераций обучения. Флуктуации около максимального результата --- это нормально. Требуемая точность оценки значений \verb|b*| и \verb|r*| --- один знак после запятой.

В отчет к этому пункту включите следующую информацию:

\begin{itemize}
    \item График обучения для полученных значений \verb|b*| и \verb|r*|. По желанию, это может быть как результат на одной случайной инициализации (random seed), так и среднее по нескольким запускам.
    \item Точную команду, использованную вами для запуска эксперимента.
\end{itemize}

\subsection{Нейросетевые реализации}

\hyphenation{ап-про-кси-ма-ции}
\hyphenation{до-за-пол-нить}
В этом задании от вас потребуется реализовать базовый метод с помощью аппроксимации функции полезности состояния нейронной сетью. Для этого необходимо дозаполнить оставшиеся пропуски, помеченные \verb|TODO|. В частности:

\begin{itemize}
    \item Добавить обучение данной нейронной сети в функции \verb|update| класса \verb|MLPPolicyPG| к уже реализованному обучению стратегии.
    \item Добавить вычисление оценки преимущества в \verb|pg_agent.py:estimate_advantage| путем вычитания из ``reward-to-go'' оценки отдачи обученной оценки полезности состояния: $\left( \sum_{t' = t}^{T - 1} {\gamma^{t'-t} r(s_{it'}, a_{it'})} \right) - V_\phi^\pi (s_{it})$.
\end{itemize}

\subsection{Сложные эксперименты}

\hyphenation{вы-чис-ли-тель-но-го}
\textit{Обратите внимание, что обучение в данной части потребует значительного вычислительного времени, поэтому, пожалуйста, не оттягивайте с началом! Для всех оставшихся экспериментов используйте ``reward-to-go'' версию оценки отдачи.}

\hyphenation{про-тес-ти-руй-те}
\textbf{Эксперимент 3: LunarLander.} Используйте градиент стратегии для обучения оптимальному управлению в среде \verb|LunarLanderContinuous-v2|. В данной части протестируйте вашу реализацию базового метода.

\hyphenation{сек-ции}
Запустите эксперимент (см. команду соответствующей  третьему эксперименту секции \verb|README|). Постройте график кривой обучения. \textit{Ожидается, что средняя отдача к концу обучения окажется примерно на уровне 180.}

\hyphenation{дли-тель-ность}
\textbf{Эксперимент 4: HalfCheetah.} Используйте градиент стратегии для обучения оптимальному управлению в среде \verb|HalfCheetah-v2|. Используйте ограничение на длительность эпизода 150 вместо 1000 по умолчанию, чтобы существенно сократить время обучения.

Проведите серию экспериментов по поиску размера пакета $b \in [10000, 30000, 50000]$ и величины коэффициента скорости обучения $r \in [0.005, 0.01, 0.02]$ (см. команду в первой части секции 4-го эксперимента в \verb|README|). Постройте единственный график, на котором будут отображены полученные кривые обучения. Опишите словами, эти два гиперпараметра влияют на результаты.

Получив наилучшие значения \verb|b*| и \verb|r*|, используйте их для проведения следующей серии экспериментов (см. список команд во второй части секции 4-го эксперимента \verb|README|). Постройте единственный график с кривыми обучения на полученной серии запусков. \textit{Ожидается, что вариант с ``reward-to-go'' и вариант с базовым методом оба получат результаты, близкие к 200.}

\subsection{Реализация обобщенной оценки преимущества (GAE)}

\hyphenation{обоб-щен-ной}
Данная часть задания посвящена реализации упрощенной версии алгоритма обобщенной оценки преимущества (GAE-$\lambda$).

Данный алгоритм по своей идее близок к алгоритму TD($\lambda$), только вместо оценки функции полезности состояния в нем оценивается функция преимущества:

\begin{equation}
    A_{GAE}^\pi (s_t, a_t) = \sum_{k=0}^{\infty} {(\gamma \lambda)^k \delta_{t+k}^V} = \sum_{k=0}^{\infty} {(\gamma \lambda)^k [r_{t+k} + \gamma V(s_{t+k+1}) - V(s_{t+k})]}
\end{equation}

\noindent
Параметр $\lambda$ здесь используется для подсчета взвешенной суммы $n$-шаговых Монте-Карло оценок преимущества (для разных $n$). Значению $\lambda = 0$ соответствует смещенная оценка с низкой дисперсией: $r_t + \gamma V(s_{t+1}) - V(s_t)$, по аналогии с TD(0), а значению $\lambda = 1$ --- несмещенная Монте-Карло оценка с высокой дисперсией: $\sum_{k=0}^{\infty} {\gamma^k r_{t+k}} - V(s_t)$. Таким образом, подбирая данный параметр, можно найти баланс между смещением и дисперсией оценки преимущества. Детали данного метода приведены в \href{https://arxiv.org/pdf/1506.02438.pdf}{оригинальной статье}.

Заполните недостающую часть реализации в \verb|pg_agent.py:estimate_advantage|.

\textbf{Experiment 5: Hopper}. Используйте полученную версию градиента стратегии с обобщенной оценкой преимущества для обучения оптимальному управлению агентом в среде \verb|Hopper-v2|. Обучите агента с разными значениями параметра $\lambda \in [0, 0.95, 0.99, 1]$ (см. команду запуска в соответствующей секции \verb|README|). \textit{Обратите внимание, что при корректной реализации GAE-$\lambda$ случай $\lambda = 1$ эквивалентен случаю с ``vanilla'' оценкой отдачи с обученным базовым методом.}

Приведите единственный график кривых обучения для проведенных вами запусков. Опишите словами, как параметр $\lambda$ повлиял на качество обучения. \textit{Ожидается, что наилучший результат обучения окажется в районе 400}.

\subsection{Дополнительные эксперименты}

В качестве бонусного задания выполните любой из пунктов ниже (или оба):

\hyphenation{пос-ле-до-ва-тель-но}
\hyphenation{рас-па-рал-ле-лить}
\hyphenation{вы-пол-ня-ем}
\hyphenation{даль-ше}
\hyphenation{гра-ди-ент-но-го}
\begin{itemize}
    \item Получение опыта в среде зачастую является одним из важнейших вычислительно затратных мест. В \verb|infrastructure/rl_trainer.py| траектории собираются последовательно в однопоточном режиме, однако этот процесс возможно распараллелить по нескольким потокам. Реализуйте многопоточную версию сбора опыта в среде и напишите краткий отчет с графиками о том, какой разницы в скорости вам удалось достичь.
    \item В методе градиента стратегии мы собираем один пакет данных для обучения, выполняем шаг градиентного спуска и затем отбрасываем эти данные, двигаясь дальше. Можем ли мы потенциально ускорить метод, делая несколько шагов градиентного спуска на одном пакете данных? Опробуйте этот вариант и напишите отчет с графиками о полученных результатах. Проведите сравнение на одной из использованных в задании MuJoCo сред.
\end{itemize}

Не забывайте включать команды запуска экспериментов в отчет, чтобы вашу работу можно было воспроизвести при проверке.

\section{Формат отправки}

\textit{Формат сдачи совпадает с форматом сдачи первого практического задания.}

\hyphenation{пред-ло-же-нии}
Сдача предполагается в виде коммитов и комментариев к ним в соответствующем предложении изменения кода (pull request) в GitHub classroom (ссылка в тг канале курса).

\hyphenation{не-до-ста-ю-щих}
Ожидается, что сдача будет содержать непосредственно код заполненных вами недостающих частей выданной заготовки решения и логи финальных запусков (для каждого задания и каждой из использованных сред).

\hyphenation{от-лич-ной}
Оригинально все логи лежат в папке \verb|data|. Логи финальных запусков скопируйте из \verb|data| в отдельную папку \verb|run_logs| и отправьте вместе с вашим решением. Отличной альтернативой будет вместо этого снабдить решение ссылкой на отчет (report) в wandb, что потребует самостоятельно интегрировать использование данного сервиса в ваше решение.

\hyphenation{ре-ше-ние}
\hyphenation{аль-тер-на-ти-ва}
Также в сообщении к предложению необходимо добавить результаты, описание и решение по каждому из пунктов задания (в соответствии с тем, что оно требует). Разметка markdown позволяет и вставку картинок, и оформление табличек. Опционально, вы можете оформить результаты в виде отдельного файла .doc или .pdf и добавить их в посылку (commit), а в сообщении сослаться на этот файл. Не забудьте для каждого пункта задания код запуска, чтобы можно было воспроизвести ваши результаты.

\end{document}
